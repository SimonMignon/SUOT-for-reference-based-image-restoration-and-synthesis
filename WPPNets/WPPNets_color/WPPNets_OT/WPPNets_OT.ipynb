{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7591050f",
   "metadata": {},
   "source": [
    "# WPPNets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a73b47",
   "metadata": {},
   "source": [
    "Ce notebook restore des images à l'aide de WPPNets par transport optimal équilibré puis semi-déséquilibré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656e527",
   "metadata": {},
   "source": [
    "# Pré-entrainé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35e08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Superresolution for the texture wood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# This code belongs to the paper\n",
    "#\n",
    "# F. Altekrüger and J. Hertrich. \n",
    "# WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution. \n",
    "# ArXiv Preprint#2201.08157\n",
    "#\n",
    "# Please cite the paper, if you use the code.\n",
    "#\n",
    "# The script reproduces the numerical example with the textures 'Floor'\n",
    "# and 'Grass' in the paper.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as io\n",
    "import model.small_acnet\n",
    "import random\n",
    "import utils\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "def Downsample(scale = 0.25, gaussian_std = 2):\n",
    "    ''' \n",
    "    downsamples an img by factor 4 using gaussian downsample from utils.py\n",
    "    '''\n",
    "    if scale > 1:\n",
    "        print('Error. Scale factor is larger than 1.')\n",
    "        return\n",
    "    gaussian_std = gaussian_std\n",
    "    kernel_size = 16\n",
    "    gaussian_down = utils.gaussian_downsample(kernel_size,gaussian_std,int(1/scale),pad=True) #gaussian downsample with zero padding\n",
    "    return gaussian_down.to(DEVICE)\n",
    "\n",
    "def WLoss(args, input_img, ref_pat, model, psi):\n",
    "    '''\n",
    "    Computes the proposed wasserstein loss fct consisting of a MSELoss and a Wasserstein regularizer\n",
    "    '''\n",
    "    lam = args.lam\n",
    "    n_patches_out = args.n_patches_out\n",
    "    patch_size = args.patch_size\n",
    "    n_iter_psi = args.n_iter_psi\n",
    "    keops = args.keops\n",
    "    \n",
    "    im2patch = utils.patch_extractor(patch_size,center=args.center)\n",
    "    \n",
    "    num_ref = ref_pat.shape[0] #number of patches of reference image\n",
    "    patch_weights = torch.ones(num_ref,device=DEVICE,dtype=torch.float) #same weight for all patches\n",
    "    \n",
    "    semidual_loss = utils.semidual(ref_pat,usekeops=keops) \n",
    "    semidual_loss.psi.data = psi #update the maximizer psi from previous step\n",
    "    pred = model(input_img) #superresolution of input_img\n",
    "    \n",
    "    #sum up all patches of whole batch\n",
    "    inp_pat = torch.empty(0, device = DEVICE)\n",
    "    for k in range(pred.shape[0]):\n",
    "        inp = im2patch(pred[k,:,:,:].unsqueeze(0)) #use all patches of input_img\n",
    "        inp_pat = torch.cat([inp_pat,inp],0)\n",
    "    inp = inp_pat\n",
    "    \n",
    "    #gradient ascent to find maximizer psi for dual formulation of W2^2\n",
    "    optim_psi = torch.optim.ASGD([semidual_loss.psi], lr=1e-0, alpha=0.5, t0=1)\n",
    "    for i in range(n_iter_psi):\n",
    "        sem = -semidual_loss(inp,patch_weights)\n",
    "        optim_psi.zero_grad()\n",
    "        sem.backward(retain_graph=True)\n",
    "        optim_psi.step()\n",
    "    semidual_loss.psi.data = optim_psi.state[semidual_loss.psi]['ax']\n",
    "    psi = semidual_loss.psi.data #update psi\n",
    "    \n",
    "    reg = semidual_loss(inp,patch_weights) #wasserstein regularizer \n",
    "    \n",
    "    down_pred = operator(pred) #downsample pred by scale_factor\n",
    "\n",
    "    loss_fct = nn.MSELoss()\n",
    "    loss = loss_fct(down_pred,input_img) #||f(G(y)) - y||^2\n",
    "    total_loss = loss + lam * reg\n",
    "    \n",
    "    return [total_loss,loss,lam*reg,psi]\n",
    "\n",
    "\n",
    "def training(trainset, model, reference_img, batch_size, epochs, args, opti):\n",
    "    '''\n",
    "    training process\n",
    "    '''\n",
    "    numb_train_img = trainset.shape[0] #number of all img\n",
    "    \n",
    "    #create random batches:\n",
    "    idx = torch.randperm(numb_train_img)\n",
    "    batch_lr = [] #list of batches\n",
    "    for i in range(0,numb_train_img,batch_size):\n",
    "        batch_lr.append(trainset[i:(i+batch_size),...])\n",
    "    \n",
    "    #create maximizer psi\n",
    "    psi_length = args.n_patches_out #length of vector psi\n",
    "    psi_list = []\n",
    "    for i in range(len(batch_lr)):\n",
    "        psi_list.append(torch.zeros(psi_length, device = DEVICE)) #create a list consisting of psi\n",
    "\n",
    "    #create random patches of reference image\n",
    "    im2patch = utils.patch_extractor(args.patch_size,center=args.center)\n",
    "    ref = im2patch(reference_img,args.n_patches_out)\n",
    "    \n",
    "    a_psnr_list = [] #for validation\n",
    "    loss_list = []; reg_list = []; MSE_list = [] #for plot\n",
    "\n",
    "    for t in tqdm(range(epochs)):\n",
    "        a_totalloss = 0; a_MSE = 0; a_reg = 0\n",
    "        ints = random.sample(range(0,len(batch_lr)),len(batch_lr)) #random order of batches\n",
    "        for i in tqdm(ints):\n",
    "            psi_temp = psi_list[i] #choose corresponding saved maximizer psi  \n",
    "            [total_loss,loss,reg,p] = WLoss(args, batch_lr[i], ref, model, psi_temp)  \n",
    "    \n",
    "            #backpropagation\n",
    "            opti.zero_grad()\n",
    "            total_loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "            total_loss = total_loss.item(); loss = loss.item(); reg = reg.item()\n",
    "            a_totalloss += total_loss; a_MSE += loss; a_reg += reg\n",
    "            psi_list[i] = p #update psi\n",
    "\n",
    "        a_totalloss = a_totalloss/len(batch_lr); a_MSE = a_MSE/len(batch_lr); a_reg = a_reg/len(batch_lr)\n",
    "        loss_list.append(a_totalloss); MSE_list.append(a_MSE); reg_list.append(a_reg)\n",
    "        \n",
    "        if not os.path.isdir('checkpoints'):\n",
    "            os.mkdir('checkpoints')\n",
    "        \n",
    "        val_step = 10\n",
    "        if (t+1)%val_step == 0:\n",
    "            print(f'------------------------------- \\nValidation step')\n",
    "            val_len = len(args.val)\n",
    "            a_psnr = 0\n",
    "            for i in range(val_len):\n",
    "                with torch.no_grad():\n",
    "                    pred = net(args.val[i][0])\n",
    "                psnr_val = utils.psnr(pred,args.val[i][1],40)\n",
    "                a_psnr += psnr_val\n",
    "            a_psnr = a_psnr / val_len\n",
    "            print(f'Average Validation PSNR: {a_psnr}')    \n",
    "            a_psnr_list.append(a_psnr)\n",
    "            plt.plot(list(range(val_step,val_step*len(a_psnr_list)+val_step,val_step)),a_psnr_list, 'k')\n",
    "            title = 'Avarage PSNR ' + str(round(a_psnr,2))\n",
    "            plt.title(title)\n",
    "            plt.savefig('checkpoints/ValidatonPSNR_'+image_class+'.pdf')\n",
    "            plt.close()\n",
    "            print(f'-------------------------------')\n",
    "        \n",
    "        #save a checkpoint\n",
    "        if (t+1)%30 == 0:\n",
    "            torch.save({'net_state_dict': model.state_dict()}, 'checkpoints/checkpoint_'+image_class+'.pth')\n",
    "            with torch.no_grad():\n",
    "                pred_hr = model(lr)\n",
    "            if not os.path.isdir('checkpoints/tmp'):\n",
    "                os.mkdir('checkpoints/tmp')\n",
    "            utils.save_img(pred_hr,'checkpoints/tmp/pred'+str(t+1))\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.plot(list(range(len(loss_list))), loss_list, 'k-.', label='avarage loss')\n",
    "            plt.plot(list(range(len(MSE_list))), MSE_list, 'k-', label='avarage MSE')\n",
    "            plt.plot(list(range(len(reg_list))), reg_list, 'k:', label='avarage Reg')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.yscale('log')\n",
    "            plt.savefig('checkpoints/losscurve_'+image_class+'.pdf')\n",
    "            plt.close()\n",
    "\n",
    "retrain = False\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.isdir('results'):\n",
    "       os.mkdir('results')    \n",
    "    \n",
    "    net = model.small_acnet.Net(scale=4).to(device=DEVICE)\n",
    "    image_classes = ['tile','wood']  \n",
    "    image_class = image_classes[1] #choose the texture\n",
    "    print('Superresolution for the texture ' + image_class)\n",
    "    \n",
    "    hr = utils.imread('test_img/hr_'+image_class+'.png')\n",
    "    lr = utils.imread('test_img/lr_'+image_class+'.png')\n",
    "    #  = operator(hr) + 0.01*torch.randn_like(operator(hr))\n",
    "    if retrain:\n",
    "        #inputs\n",
    "        lr_train = utils.Trainset(image_class = image_class, size = 1000)\n",
    "        val = utils.Validationset(image_class = image_class)\t\n",
    "        lr_size = lr_train.shape[2]\n",
    "        operator = Downsample(scale = 1/4, gaussian_std = 2)\n",
    "\n",
    "        args=argparse.Namespace()\n",
    "        args.lam=12.5/(3*lr_size**2)\n",
    "        args.n_patches_out=10000\n",
    "        args.patch_size=6\n",
    "        args.val = val\n",
    "        args.keops = True\n",
    "        if image_class == 'tile':\n",
    "            args.center = True\n",
    "            epochs = 2\n",
    "            args.n_iter_psi=20\n",
    "        elif image_class == 'wood':\n",
    "            args.center = True\n",
    "            epochs = 2\n",
    "            args.n_iter_psi=20\n",
    "            \n",
    "        reference_img = utils.imread('test_img/ref_'+image_class+'.png')        \n",
    "        \n",
    "        #training process\n",
    "        batch_size = 25\n",
    "        learning_rate = 1e-4\n",
    "        OPTIMIZER = torch.optim.Adam(net.parameters(), lr=learning_rate)    \n",
    "        \n",
    "        training(lr_train,net,reference_img,batch_size,epochs,args=args,opti=OPTIMIZER)\n",
    "        with torch.no_grad():\n",
    "            pred = net(lr)\n",
    "        torch.save({'net_state_dict': net.state_dict(), 'optimizer_state_dict': OPTIMIZER.state_dict()},\n",
    "                    'results/weights_'+image_class+'.pth')        \n",
    "        utils.save_img(pred,'results/W2_'+image_class)\n",
    "            \n",
    "    if not retrain:\n",
    "        weights = torch.load('results/weights_'+image_class+'.pth',map_location=DEVICE)\n",
    "        net.load_state_dict(weights['net_state_dict'])\n",
    "        pred = net(lr)\n",
    "        utils.save_img(pred,'results/W2_'+image_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13aa085",
   "metadata": {},
   "source": [
    "# WOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a616eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6850, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# PSNR\n",
    "import torchvision\n",
    "def PSNR(im,im_new):\n",
    "    C,M,N=im_new.shape\n",
    "    EQM=1/(C*M*N)*torch.sum((im-im_new)**2)\n",
    "    psnr=10*torch.log10(1/EQM)\n",
    "    return(psnr)\n",
    "\n",
    "print(PSNR(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')),\n",
    "                                torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafe2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/prof/smignon/anaconda3/envs/WPPNets_color/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "tensor([[[[0.1625]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LPIPS\n",
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "print(loss_fn_alex(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).unsqueeze(0)\n",
    "                                          , torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765efe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67207694\n"
     ]
    }
   ],
   "source": [
    "# SSIM\n",
    "#from skimage import (color, data, measure)\n",
    "import torchvision\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "img_hr=torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "img_pred_H=torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "print(ssim(img_hr, img_pred_H,data_range=img_pred_H.max() - img_pred_H.min(),multichannel=True,channel_axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b116c4",
   "metadata": {},
   "source": [
    "# TILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c911e589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3690, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# PSNR\n",
    "import torchvision\n",
    "def PSNR(im,im_new):\n",
    "    C,M,N=im_new.shape\n",
    "    EQM=1/(C*M*N)*torch.sum((im-im_new)**2)\n",
    "    psnr=10*torch.log10(1/EQM)\n",
    "    return(psnr)\n",
    "\n",
    "print(PSNR(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')),\n",
    "                                torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8818dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/prof/smignon/anaconda3/envs/WPPNets_color/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "tensor([[[[0.2641]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LPIPS\n",
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "print(loss_fn_alex(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).unsqueeze(0)\n",
    "                                          , torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca6f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640308\n"
     ]
    }
   ],
   "source": [
    "# SSIM\n",
    "#from skimage import (color, data, measure)\n",
    "import torchvision\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "img_hr=torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "img_pred_H=torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "print(ssim(img_hr, img_pred_H,data_range=img_pred_H.max() - img_pred_H.min(),multichannel=True,channel_axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4437434",
   "metadata": {},
   "source": [
    "# PSNR, LPIPS, SSIM moyens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f36858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/prof/smignon/anaconda3/envs/WPPNets_color/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "tensor(32.0700)\n",
      "tensor(0.1743)\n",
      "0.6651316\n"
     ]
    }
   ],
   "source": [
    "# PSNR, LPIPS, SSIM, Blur Effect moyen sur une base d'images constitué d'un seul type de textures WOOD1:\n",
    "import numpy as np\n",
    "import glob\n",
    "from skimage import (color, data, measure)\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "def PSNR(im,im_new):\n",
    "    C,M,N=im_new.shape\n",
    "    EQM=1/(C*M*N)*torch.sum((im-im_new)**2)\n",
    "    psnr=10*torch.log10(1/EQM)\n",
    "    return(psnr)\n",
    "\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "\n",
    "# emplacement des images \n",
    "liste_im_name_HR   = [file for file in glob.glob(\"test_img/test_img_WPPNets_color/wood/HR/*.png\")]#.sort()\n",
    "liste_im_name_LR   = [file for file in glob.glob(\"test_img/test_img_WPPNets_color/wood/LR/*.png\")]#.sort()\n",
    "\n",
    "liste_im_name_HR.sort()\n",
    "liste_im_name_LR.sort()\n",
    "\n",
    "# listes pour enregistrer PSNR, LPIPS, SSIM, Blur Effect \n",
    "PSNRs=[]\n",
    "LPIPS=[]\n",
    "SSIM=[]\n",
    "Blue_Effect=[]\n",
    "PRED=[]\n",
    "\n",
    "# Chargement du bon NN\n",
    "weights = torch.load('results/weights_wood.pth',map_location=DEVICE)\n",
    "net.load_state_dict(weights['net_state_dict'])\n",
    "\n",
    "for i,name_im in enumerate(liste_im_name_HR):\n",
    "    hr = utils.imread(liste_im_name_HR[i])\n",
    "    lr = utils.imread(liste_im_name_LR[i])\n",
    "    pred = net(lr)\n",
    "    \n",
    "    # LPIPS\n",
    "    LPIPS.append(loss_fn_alex(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).unsqueeze(0)\n",
    "                                          ,torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).unsqueeze(0)))\n",
    "    # PSNR\n",
    "    PSNRs.append(PSNR(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')),\n",
    "                                torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu'))))\n",
    "    \n",
    "    # SSIM\n",
    "    img_hr=torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "    img_pred_H=torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "    SSIM.append(ssim(img_hr, img_pred_H,data_range=img_pred_H.max() - img_pred_H.min(),multichannel=True,channel_axis=-1))\n",
    "    \n",
    "    # Blur Effect\n",
    "    #Blue_Effect.append(measure.blur_effect(img_pred, h_size=11))\n",
    "    \n",
    "    # im pred \n",
    "    PRED.append(pred)\n",
    "    \n",
    "# Moyenne des valeurs sur le jeu de données\n",
    "print(torch.mean(torch.tensor(PSNRs)))\n",
    "print(torch.mean(torch.tensor(LPIPS)))\n",
    "print(np.mean(SSIM))\n",
    "#print(np.mean(Blue_Effect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1e0752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/prof/smignon/anaconda3/envs/WPPNets_color/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "tensor(33.8322)\n",
      "tensor(0.2282)\n",
      "0.87470067\n"
     ]
    }
   ],
   "source": [
    "# PSNR, LPIPS, SSIM, Blur Effect moyen sur une base d'images constitué d'un seul type de textures WOOD1:\n",
    "import numpy as np\n",
    "import glob\n",
    "from skimage import (color, data, measure)\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def PSNR(im,im_new):\n",
    "    C,M,N=im_new.shape\n",
    "    EQM=1/(C*M*N)*torch.sum((im-im_new)**2)\n",
    "    psnr=10*torch.log10(1/EQM)\n",
    "    return(psnr)\n",
    "\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "\n",
    "# emplacement des images \n",
    "liste_im_name_HR   = [file for file in glob.glob(\"test_img/test_img_WPPNets_color/tile/HR/*.png\")]#.sort()\n",
    "liste_im_name_LR   = [file for file in glob.glob(\"test_img/test_img_WPPNets_color/tile/LR/*.png\")]#.sort()\n",
    "\n",
    "liste_im_name_HR.sort()\n",
    "liste_im_name_LR.sort()\n",
    "\n",
    "# listes pour enregistrer PSNR, LPIPS, SSIM, Blur Effect \n",
    "PSNRs=[]\n",
    "LPIPS=[]\n",
    "SSIM=[]\n",
    "Blue_Effect=[]\n",
    "PRED=[]\n",
    "\n",
    "# Chargement du bon NN\n",
    "weights = torch.load('results/weights_tile.pth',map_location=DEVICE)\n",
    "net.load_state_dict(weights['net_state_dict'])\n",
    "\n",
    "for i,name_im in enumerate(liste_im_name_HR):\n",
    "    hr = utils.imread(liste_im_name_HR[i])\n",
    "    lr = utils.imread(liste_im_name_LR[i])\n",
    "    pred = net(lr)\n",
    "    \n",
    "    # LPIPS\n",
    "    LPIPS.append(loss_fn_alex(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).unsqueeze(0)\n",
    "                                          ,torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).unsqueeze(0)))\n",
    "    # PSNR\n",
    "    PSNRs.append(PSNR(torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')),\n",
    "                                torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu'))))\n",
    "    \n",
    "    # SSIM\n",
    "    img_hr=torchvision.transforms.CenterCrop(600-12)(hr.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "    img_pred_H=torchvision.transforms.CenterCrop(600-12)(pred.squeeze().to('cpu')).detach().numpy().transpose(1, 2, 0)\n",
    "    SSIM.append(ssim(img_hr, img_pred_H,data_range=img_pred_H.max() - img_pred_H.min(),multichannel=True,channel_axis=-1))\n",
    "    \n",
    "    # Blur Effect\n",
    "    #Blue_Effect.append(measure.blur_effect(img_pred, h_size=11))\n",
    "    \n",
    "    # im pred \n",
    "    PRED.append(pred)\n",
    "    \n",
    "# Moyenne des valeurs sur le jeu de données\n",
    "print(torch.mean(torch.tensor(PSNRs)))\n",
    "print(torch.mean(torch.tensor(LPIPS)))\n",
    "print(np.mean(SSIM))\n",
    "#print(np.mean(Blue_Effect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42097587",
   "metadata": {},
   "source": [
    "# À entrainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab1e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Superresolution for the texture tile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|█                                           | 1/40 [00:03<02:35,  3.99s/it]\u001b[A\n",
      "  5%|██▏                                         | 2/40 [00:06<01:49,  2.88s/it]\u001b[A\n",
      "  8%|███▎                                        | 3/40 [00:08<01:32,  2.49s/it]\u001b[A\n",
      " 10%|████▍                                       | 4/40 [00:10<01:22,  2.28s/it]\u001b[A\n",
      " 12%|█████▌                                      | 5/40 [00:12<01:15,  2.16s/it]\u001b[A\n",
      " 15%|██████▌                                     | 6/40 [00:13<01:09,  2.04s/it]\u001b[A\n",
      " 18%|███████▋                                    | 7/40 [00:15<01:04,  1.95s/it]\u001b[A\n",
      " 20%|████████▊                                   | 8/40 [00:17<00:59,  1.87s/it]\u001b[A\n",
      " 22%|█████████▉                                  | 9/40 [00:18<00:55,  1.80s/it]\u001b[A\n",
      " 25%|██████████▊                                | 10/40 [00:20<00:52,  1.74s/it]\u001b[A\n",
      " 28%|███████████▊                               | 11/40 [00:22<00:49,  1.70s/it]\u001b[A\n",
      " 30%|████████████▉                              | 12/40 [00:23<00:46,  1.67s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 13/40 [00:25<00:44,  1.64s/it]\u001b[A\n",
      " 35%|███████████████                            | 14/40 [00:26<00:42,  1.62s/it]\u001b[A\n",
      " 38%|████████████████▏                          | 15/40 [00:28<00:40,  1.60s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 16/40 [00:30<00:38,  1.58s/it]\u001b[A\n",
      " 42%|██████████████████▎                        | 17/40 [00:31<00:36,  1.58s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 18/40 [00:33<00:34,  1.58s/it]\u001b[A\n",
      " 48%|████████████████████▍                      | 19/40 [00:34<00:33,  1.57s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 20/40 [00:36<00:31,  1.57s/it]\u001b[A\n",
      " 52%|██████████████████████▌                    | 21/40 [00:37<00:29,  1.57s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 22/40 [00:39<00:28,  1.57s/it]\u001b[A\n",
      " 57%|████████████████████████▋                  | 23/40 [00:40<00:26,  1.57s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 24/40 [00:42<00:25,  1.57s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 25/40 [00:44<00:23,  1.57s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 26/40 [00:45<00:21,  1.56s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 27/40 [00:47<00:20,  1.57s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 28/40 [00:48<00:18,  1.56s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 29/40 [00:50<00:17,  1.56s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 30/40 [00:51<00:15,  1.55s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 31/40 [00:53<00:14,  1.56s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 32/40 [00:55<00:12,  1.56s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 33/40 [00:56<00:10,  1.56s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 34/40 [00:58<00:09,  1.56s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 35/40 [00:59<00:07,  1.57s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 36/40 [01:01<00:06,  1.56s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▊   | 37/40 [01:02<00:04,  1.57s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 38/40 [01:04<00:03,  1.56s/it]\u001b[A\n",
      " 98%|█████████████████████████████████████████▉ | 39/40 [01:05<00:01,  1.56s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 40/40 [01:07<00:00,  1.69s/it]\u001b[A\n",
      " 50%|██████████████████████▌                      | 1/2 [01:07<01:07, 67.49s/it]\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|█                                           | 1/40 [00:01<01:01,  1.57s/it]\u001b[A\n",
      "  5%|██▏                                         | 2/40 [00:03<00:59,  1.57s/it]\u001b[A\n",
      "  8%|███▎                                        | 3/40 [00:04<00:58,  1.57s/it]\u001b[A\n",
      " 10%|████▍                                       | 4/40 [00:06<00:56,  1.57s/it]\u001b[A\n",
      " 12%|█████▌                                      | 5/40 [00:07<00:54,  1.57s/it]\u001b[A\n",
      " 15%|██████▌                                     | 6/40 [00:09<00:53,  1.57s/it]\u001b[A\n",
      " 18%|███████▋                                    | 7/40 [00:10<00:51,  1.57s/it]\u001b[A\n",
      " 20%|████████▊                                   | 8/40 [00:12<00:50,  1.57s/it]\u001b[A\n",
      " 22%|█████████▉                                  | 9/40 [00:14<00:48,  1.57s/it]\u001b[A\n",
      " 25%|██████████▊                                | 10/40 [00:15<00:46,  1.56s/it]\u001b[A\n",
      " 28%|███████████▊                               | 11/40 [00:17<00:45,  1.56s/it]\u001b[A\n",
      " 30%|████████████▉                              | 12/40 [00:18<00:43,  1.57s/it]\u001b[A\n",
      " 32%|█████████████▉                             | 13/40 [00:20<00:42,  1.56s/it]\u001b[A\n",
      " 35%|███████████████                            | 14/40 [00:21<00:40,  1.55s/it]\u001b[A\n",
      " 38%|████████████████▏                          | 15/40 [00:23<00:38,  1.56s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 16/40 [00:24<00:37,  1.55s/it]\u001b[A\n",
      " 42%|██████████████████▎                        | 17/40 [00:26<00:35,  1.55s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 18/40 [00:28<00:34,  1.55s/it]\u001b[A\n",
      " 48%|████████████████████▍                      | 19/40 [00:29<00:32,  1.56s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 20/40 [00:31<00:31,  1.56s/it]\u001b[A\n",
      " 52%|██████████████████████▌                    | 21/40 [00:32<00:29,  1.56s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 22/40 [00:34<00:28,  1.56s/it]\u001b[A\n",
      " 57%|████████████████████████▋                  | 23/40 [00:35<00:26,  1.56s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 24/40 [00:37<00:25,  1.57s/it]\u001b[A\n",
      " 62%|██████████████████████████▉                | 25/40 [00:39<00:23,  1.57s/it]\u001b[A\n",
      " 65%|███████████████████████████▉               | 26/40 [00:40<00:22,  1.57s/it]\u001b[A\n",
      " 68%|█████████████████████████████              | 27/40 [00:42<00:20,  1.57s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 28/40 [00:43<00:18,  1.57s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 29/40 [00:45<00:17,  1.56s/it]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 30/40 [00:46<00:15,  1.57s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 31/40 [00:48<00:14,  1.57s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 32/40 [00:50<00:12,  1.57s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 33/40 [00:51<00:11,  1.57s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 34/40 [00:53<00:09,  1.56s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 35/40 [00:54<00:07,  1.57s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 36/40 [00:56<00:06,  1.57s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▊   | 37/40 [00:57<00:04,  1.56s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 38/40 [00:59<00:03,  1.56s/it]\u001b[A\n",
      " 98%|█████████████████████████████████████████▉ | 39/40 [01:01<00:01,  1.57s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 40/40 [01:02<00:00,  1.56s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:10<00:00, 65.03s/it]\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# This code belongs to the paper\n",
    "#\n",
    "# F. Altekrüger and J. Hertrich. \n",
    "# WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution. \n",
    "# ArXiv Preprint#2201.08157\n",
    "#\n",
    "# Please cite the paper, if you use the code.\n",
    "#\n",
    "# The script reproduces the numerical example with the textures 'Floor'\n",
    "# and 'Grass' in the paper.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as io\n",
    "import model.small_acnet\n",
    "import random\n",
    "import utils\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "def Downsample(scale = 0.25, gaussian_std = 2):\n",
    "    ''' \n",
    "    downsamples an img by factor 4 using gaussian downsample from utils.py\n",
    "    '''\n",
    "    if scale > 1:\n",
    "        print('Error. Scale factor is larger than 1.')\n",
    "        return\n",
    "    gaussian_std = gaussian_std\n",
    "    kernel_size = 16\n",
    "    gaussian_down = utils.gaussian_downsample(kernel_size,gaussian_std,int(1/scale),pad=True) #gaussian downsample with zero padding\n",
    "    return gaussian_down.to(DEVICE)\n",
    "\n",
    "def WLoss(args, input_img, ref_pat, model, psi):\n",
    "    '''\n",
    "    Computes the proposed wasserstein loss fct consisting of a MSELoss and a Wasserstein regularizer\n",
    "    '''\n",
    "    lam = args.lam\n",
    "    n_patches_out = args.n_patches_out\n",
    "    patch_size = args.patch_size\n",
    "    n_iter_psi = args.n_iter_psi\n",
    "    keops = args.keops\n",
    "    \n",
    "    im2patch = utils.patch_extractor(patch_size,center=args.center)\n",
    "    \n",
    "    num_ref = ref_pat.shape[0] #number of patches of reference image\n",
    "    patch_weights = torch.ones(num_ref,device=DEVICE,dtype=torch.float) #same weight for all patches\n",
    "    \n",
    "    semidual_loss = utils.semidual(ref_pat,usekeops=keops) \n",
    "    semidual_loss.psi.data = psi #update the maximizer psi from previous step\n",
    "    pred = model(input_img) #superresolution of input_img\n",
    "    \n",
    "    #sum up all patches of whole batch\n",
    "    inp_pat = torch.empty(0, device = DEVICE)\n",
    "    for k in range(pred.shape[0]):\n",
    "        inp = im2patch(pred[k,:,:,:].unsqueeze(0)) #use all patches of input_img\n",
    "        inp_pat = torch.cat([inp_pat,inp],0)\n",
    "    inp = inp_pat\n",
    "    \n",
    "    #gradient ascent to find maximizer psi for dual formulation of W2^2\n",
    "    optim_psi = torch.optim.ASGD([semidual_loss.psi], lr=1e-0, alpha=0.5, t0=1)\n",
    "    for i in range(n_iter_psi):\n",
    "        sem = -semidual_loss(inp,patch_weights)\n",
    "        optim_psi.zero_grad()\n",
    "        sem.backward(retain_graph=True)\n",
    "        optim_psi.step()\n",
    "    semidual_loss.psi.data = optim_psi.state[semidual_loss.psi]['ax']\n",
    "    psi = semidual_loss.psi.data #update psi\n",
    "    \n",
    "    reg = semidual_loss(inp,patch_weights) #wasserstein regularizer \n",
    "    \n",
    "    down_pred = operator(pred) #downsample pred by scale_factor\n",
    "\n",
    "    loss_fct = nn.MSELoss()\n",
    "    loss = loss_fct(down_pred,input_img) #||f(G(y)) - y||^2\n",
    "    total_loss = loss + lam * reg\n",
    "    \n",
    "    return [total_loss,loss,lam*reg,psi]\n",
    "\n",
    "\n",
    "def training(trainset, model, reference_img, batch_size, epochs, args, opti):\n",
    "    '''\n",
    "    training process\n",
    "    '''\n",
    "    numb_train_img = trainset.shape[0] #number of all img\n",
    "    \n",
    "    #create random batches:\n",
    "    idx = torch.randperm(numb_train_img)\n",
    "    batch_lr = [] #list of batches\n",
    "    for i in range(0,numb_train_img,batch_size):\n",
    "        batch_lr.append(trainset[i:(i+batch_size),...])\n",
    "    \n",
    "    #create maximizer psi\n",
    "    psi_length = args.n_patches_out #length of vector psi\n",
    "    psi_list = []\n",
    "    for i in range(len(batch_lr)):\n",
    "        psi_list.append(torch.zeros(psi_length, device = DEVICE)) #create a list consisting of psi\n",
    "\n",
    "    #create random patches of reference image\n",
    "    im2patch = utils.patch_extractor(args.patch_size,center=args.center)\n",
    "    ref = im2patch(reference_img,args.n_patches_out)\n",
    "    \n",
    "    a_psnr_list = [] #for validation\n",
    "    loss_list = []; reg_list = []; MSE_list = [] #for plot\n",
    "\n",
    "    for t in tqdm(range(epochs)):\n",
    "        a_totalloss = 0; a_MSE = 0; a_reg = 0\n",
    "        ints = random.sample(range(0,len(batch_lr)),len(batch_lr)) #random order of batches\n",
    "        for i in tqdm(ints):\n",
    "            psi_temp = psi_list[i] #choose corresponding saved maximizer psi  \n",
    "            [total_loss,loss,reg,p] = WLoss(args, batch_lr[i], ref, model, psi_temp)  \n",
    "    \n",
    "            #backpropagation\n",
    "            opti.zero_grad()\n",
    "            total_loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "            total_loss = total_loss.item(); loss = loss.item(); reg = reg.item()\n",
    "            a_totalloss += total_loss; a_MSE += loss; a_reg += reg\n",
    "            psi_list[i] = p #update psi\n",
    "\n",
    "        a_totalloss = a_totalloss/len(batch_lr); a_MSE = a_MSE/len(batch_lr); a_reg = a_reg/len(batch_lr)\n",
    "        loss_list.append(a_totalloss); MSE_list.append(a_MSE); reg_list.append(a_reg)\n",
    "        \n",
    "        if not os.path.isdir('checkpoints'):\n",
    "            os.mkdir('checkpoints')\n",
    "        \n",
    "        val_step = 10\n",
    "        if (t+1)%val_step == 0:\n",
    "            print(f'------------------------------- \\nValidation step')\n",
    "            val_len = len(args.val)\n",
    "            a_psnr = 0\n",
    "            for i in range(val_len):\n",
    "                with torch.no_grad():\n",
    "                    pred = net(args.val[i][0])\n",
    "                psnr_val = utils.psnr(pred,args.val[i][1],40)\n",
    "                a_psnr += psnr_val\n",
    "            a_psnr = a_psnr / val_len\n",
    "            print(f'Average Validation PSNR: {a_psnr}')    \n",
    "            a_psnr_list.append(a_psnr)\n",
    "            plt.plot(list(range(val_step,val_step*len(a_psnr_list)+val_step,val_step)),a_psnr_list, 'k')\n",
    "            title = 'Avarage PSNR ' + str(round(a_psnr,2))\n",
    "            plt.title(title)\n",
    "            plt.savefig('checkpoints/ValidatonPSNR_'+image_class+'.pdf')\n",
    "            plt.close()\n",
    "            print(f'-------------------------------')\n",
    "        \n",
    "        #save a checkpoint\n",
    "        if (t+1)%30 == 0:\n",
    "            torch.save({'net_state_dict': model.state_dict()}, 'checkpoints/checkpoint_'+image_class+'.pth')\n",
    "            with torch.no_grad():\n",
    "                pred_hr = model(lr)\n",
    "            if not os.path.isdir('checkpoints/tmp'):\n",
    "                os.mkdir('checkpoints/tmp')\n",
    "            utils.save_img(pred_hr,'checkpoints/tmp/pred'+str(t+1))\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.plot(list(range(len(loss_list))), loss_list, 'k-.', label='avarage loss')\n",
    "            plt.plot(list(range(len(MSE_list))), MSE_list, 'k-', label='avarage MSE')\n",
    "            plt.plot(list(range(len(reg_list))), reg_list, 'k:', label='avarage Reg')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.yscale('log')\n",
    "            plt.savefig('checkpoints/losscurve_'+image_class+'.pdf')\n",
    "            plt.close()\n",
    "\n",
    "retrain = True\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.isdir('results'):\n",
    "       os.mkdir('results')    \n",
    "    \n",
    "    net = model.small_acnet.Net(scale=4).to(device=DEVICE)\n",
    "    image_classes = ['tile','wood']  \n",
    "    image_class = image_classes[0] #choose the texture\n",
    "    print('Superresolution for the texture ' + image_class)\n",
    "    \n",
    "    hr = utils.imread('test_img/hr_'+image_class+'.png')\n",
    "    lr = utils.imread('test_img/lr_'+image_class+'.png')\n",
    "    #  = operator(hr) + 0.01*torch.randn_like(operator(hr))\n",
    "    if retrain:\n",
    "        #inputs\n",
    "        lr_train = utils.Trainset(image_class = image_class, size = 1000)\n",
    "        val = utils.Validationset(image_class = image_class)\t\n",
    "        lr_size = lr_train.shape[2]\n",
    "        operator = Downsample(scale = 1/4, gaussian_std = 2)\n",
    "\n",
    "        args=argparse.Namespace()\n",
    "        args.lam=12.5/(3*lr_size**2)\n",
    "        args.n_patches_out=10000\n",
    "        args.patch_size=6\n",
    "        args.val = val\n",
    "        args.keops = True\n",
    "        if image_class == 'tile':\n",
    "            args.center = True\n",
    "            epochs = 2\n",
    "            args.n_iter_psi=20\n",
    "        elif image_class == 'wood':\n",
    "            args.center = True\n",
    "            epochs = 2\n",
    "            args.n_iter_psi=20\n",
    "            \n",
    "        reference_img = utils.imread('test_img/ref_'+image_class+'.png')        \n",
    "        \n",
    "        #training process\n",
    "        batch_size = 25\n",
    "        learning_rate = 1e-4\n",
    "        OPTIMIZER = torch.optim.Adam(net.parameters(), lr=learning_rate)    \n",
    "        \n",
    "        training(lr_train,net,reference_img,batch_size,epochs,args=args,opti=OPTIMIZER)\n",
    "        with torch.no_grad():\n",
    "            pred = net(lr)\n",
    "        torch.save({'net_state_dict': net.state_dict(), 'optimizer_state_dict': OPTIMIZER.state_dict()},\n",
    "                    'results/weights_'+image_class+'.pth')        \n",
    "        utils.save_img(pred,'results/W2_'+image_class)\n",
    "            \n",
    "    if not retrain:\n",
    "        weights = torch.load('results/weights_'+image_class+'.pth',map_location=DEVICE)\n",
    "        net.load_state_dict(weights['net_state_dict'])\n",
    "        pred = net(lr)\n",
    "        utils.save_img(pred,'results/W2_'+image_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c32b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WPPNets_color",
   "language": "python",
   "name": "wppnets_color"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
